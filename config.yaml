# Internal LLM API Server Configuration

server:
  host: "0.0.0.0"
  port: 8000
  workers: 4
  log_level: "info"
  cors_enabled: true
  cors_origins:
    - "*"

# vLLM Backend Configuration
vllm:
  # vLLM OpenAI-compatible server endpoint
  base_url: "http://localhost:8100/v1"
  # Default model name (should match what's loaded in vLLM)
  default_model: "meta-llama/Llama-2-7b-chat-hf"
  # Timeout for vLLM requests (seconds)
  timeout: 300
  # Max retries for failed requests
  max_retries: 3

# API Key Configuration
# In production, use a database or secrets manager
# Format: api_key: {user_id: "user_name", tier: "tier_name"}
api_keys:
  sk-internal-dev-key-001:
    user_id: "dev-team"
    tier: "premium"
  sk-internal-test-key-002:
    user_id: "test-user"
    tier: "standard"

# Rate Limiting Configuration (requests per time window)
rate_limits:
  premium:
    requests_per_minute: 100
    requests_per_hour: 1000
  standard:
    requests_per_minute: 30
    requests_per_hour: 300
  free:
    requests_per_minute: 10
    requests_per_hour: 100

# Logging Configuration
logging:
  # Directory for log files
  log_dir: "./logs"
  # Log rotation
  rotation: "500 MB"
  # Log retention
  retention: "30 days"
  # Log request/response bodies (disable for privacy)
  log_bodies: true
  # Maximum body size to log (bytes)
  max_body_log_size: 10000

# Monitoring Configuration
monitoring:
  # Enable Prometheus metrics endpoint
  prometheus_enabled: true
  metrics_port: 9090

# Security Configuration
security:
  # Require HTTPS in production
  require_https: false
  # API key header name
  api_key_header: "Authorization"
  # Expected format: "Bearer {api_key}"
  use_bearer_format: true
