version: '3.8'

# Development docker-compose without vLLM (for testing without GPU)
# Usage: docker compose -f docker-compose.dev.yml up

services:
  # Admin Service (API Key Management)
  admin:
    build:
      context: .
      dockerfile: admin/Dockerfile
    container_name: admin-service
    ports:
      - "8002:8002"
    volumes:
      - ./llm_api.db:/app/llm_api.db
    environment:
      - DATABASE_URL=sqlite:///./llm_api.db
      - ADMIN_SECRET_KEY=change-this-secret-key-in-production
    restart: unless-stopped
    networks:
      - llm-api-network

  # LLM Backend Service (Mock - points to external vLLM or mock server)
  llm-backend:
    build:
      context: .
      dockerfile: llm_backend/Dockerfile
    container_name: llm-backend
    ports:
      - "8001:8001"
    environment:
      # Point to external vLLM server or use mock endpoint
      - VLLM_BASE_URL=http://host.docker.internal:8100
      - VLLM_DEFAULT_MODEL=meta-llama/Llama-2-7b-chat-hf
    restart: unless-stopped
    networks:
      - llm-api-network

  # Gateway Service (Auth + Routing)
  gateway:
    build:
      context: .
      dockerfile: gateway/Dockerfile
    container_name: gateway-service
    ports:
      - "8000:8000"
    volumes:
      - ./llm_api.db:/app/llm_api.db
    environment:
      - DATABASE_URL=sqlite:///./llm_api.db
      - LLM_BACKEND_URL=http://llm-backend:8001
      - ADMIN_HOST=admin
      - ADMIN_PORT=8002
      - VLLM_BASE_URL=http://host.docker.internal:8100
    depends_on:
      - llm-backend
      - admin
    restart: unless-stopped
    networks:
      - llm-api-network

networks:
  llm-api-network:
    driver: bridge
